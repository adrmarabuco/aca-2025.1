# -*- coding: utf-8 -*-
"""Q Learning - Execução.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127w6sd6piMUaT1P3w9yGNWRpJmlYjCac

**MATRIZ DE TRANSIÇÃO DE ESTADOS (PARA SIMULAÇÃO DO AGENTE)**
"""
import numpy as np
import pandas as pd
import logging
from time import sleep
import typer
from rich import print
from datetime import datetime

# Setup logging to file with timestamp
log_filename = "rl/" + datetime.now().strftime("%Y-%m-%dT%H-%M-%S.log")
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s',
    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]
)

# alfas = range(0.05, 0.5, 0.1)
    # gammas = range(1, 0.8, 0.05)
    
logging.info(f"Q Learning commencing")
logging.info("-" * 50)

# Cria uma matriz 11x11 preenchida com zeros
T_up = np.zeros((11, 11))

# Preenche a matriz com as probabilidades de transição
T_up[0, 3] = 0.1
T_up[0, 0] = 0.1
T_up[0, 1] = 0.8
T_up[1, 1] = 0.2
T_up[1, 2] = 0.8
T_up[2, 4] = 0.1
T_up[2, 2] = 0.9
T_up[3, 5] = 0.1
T_up[3, 3] = 0.8
T_up[3, 0] = 0.1
T_up[4, 7] = 0.1
T_up[4, 2] = 0.1
T_up[4, 4] = 0.8
T_up[5, 8] = 0.1
T_up[5, 3] = 0.1
T_up[5, 6] = 0.8
T_up[6, 9] = 0.1
T_up[6, 6] = 0.1
T_up[6, 7] = 0.8
T_up[7, 10] = 0.1
T_up[7, 4] = 0.1
T_up[7, 7] = 0.8
T_up[8, 8] = 0.1
T_up[8, 5] = 0.1
T_up[8, 9] = 0.8

import numpy as np

# MATRIZ DE TRANSIÇÃO PARA A AÇÃO DOWN
T_down = np.zeros((11, 11))
T_down[0, 0] = 0.9
T_down[0, 3] = 0.1
T_down[1, 0] = 0.8
T_down[1, 1] = 0.2
T_down[2, 1] = 0.8
T_down[2, 2] = 0.1
T_down[2, 4] = 0.1
T_down[3, 0] = 0.1
T_down[3, 5] = 0.1
T_down[3, 3] = 0.8
T_down[4, 4] = 0.8
T_down[4, 2] = 0.1
T_down[4, 7] = 0.1
T_down[5, 5] = 0.8
T_down[5, 3] = 0.1
T_down[5, 8] = 0.1
T_down[6, 5] = 0.8
T_down[6, 6] = 0.1
T_down[6, 9] = 0.1
T_down[7, 6] = 0.8
T_down[7, 4] = 0.1
T_down[7, 10] = 0.1
T_down[8, 8] = 0.9
T_down[8, 5] = 0.1

# MATRIZ DE TRANSIÇÃO PARA A AÇÃO LEFT
T_left = np.zeros((11, 11))
T_left[0, 0] = 0.9
T_left[0, 1] = 0.1
T_left[1, 0] = 0.1
T_left[1, 2] = 0.1
T_left[1, 1] = 0.8
T_left[2, 2] = 0.9
T_left[2, 1] = 0.1
T_left[3, 0] = 0.8
T_left[3, 3] = 0.2
T_left[4, 2] = 0.8
T_left[4, 4] = 0.2
T_left[5, 3] = 0.8
T_left[5, 5] = 0.1
T_left[5, 6] = 0.1
T_left[6, 6] = 0.8
T_left[6, 5] = 0.1
T_left[6, 7] = 0.1
T_left[7, 4] = 0.8
T_left[7, 7] = 0.1
T_left[7, 6] = 0.1
T_left[8, 5] = 0.8
T_left[8, 8] = 0.1
T_left[8, 9] = 0.1

# MATRIZ DE TRANSIÇÃO PARA A AÇÃO RIGHT
T_right = np.zeros((11, 11))
T_right[0, 3] = 0.8
T_right[0, 0] = 0.1
T_right[0, 1] = 0.1
T_right[1, 1] = 0.8
T_right[1, 0] = 0.1
T_right[1, 2] = 0.1
T_right[2, 4] = 0.8
T_right[2, 2] = 0.1
T_right[2, 1] = 0.1
T_right[3, 5] = 0.8
T_right[3, 3] = 0.2
T_right[4, 7] = 0.8
T_right[4, 4] = 0.2
T_right[5, 8] = 0.9
T_right[5, 5] = 0.1
T_right[5, 6] = 0.1
T_right[6, 9] = 0.8
T_right[6, 5] = 0.1
T_right[6, 7] = 0.1
T_right[7, 10] = 0.8
T_right[7, 6] = 0.1
T_right[7, 7] = 0.1
T_right[8, 9] = 0.1
T_right[8, 8] = 0.9

"""**FUNÇÕES AUXILIARES**"""

# Função para simular o resultado estocástico de aplicar uma ação ao estado atual

import numpy as np

def calc_action_result(state, transition_state):

    # Obtém os índices dos estados candidatos (com probabilidade > 0)
    cand_states = np.where(transition_state != 0)[0]
    prod_cand_states = transition_state[cand_states]

    # Ordena as probabilidades e obtém os índices ordenados
    sorted_indices = np.argsort(prod_cand_states)
    cand_states_sort = cand_states[sorted_indices]
    prod_cand_states_sort = prod_cand_states[sorted_indices]

    # Constrói a roleta acumulada
    roleta = np.cumsum(prod_cand_states_sort)

    # Gera número aleatório uniforme entre 0 e 1
    r = np.random.uniform()

    # Encontra o primeiro índice da roleta onde a probabilidade acumulada excede r
    ind = np.where(roleta > r)[0]

    return cand_states_sort[ind[0]]

def epsilon_greedy(q_matrix, state, epsilon):
    """
    Função que implementa a estratégia epsilon-greedy
    """
    if np.random.uniform() < epsilon:
        # Explore: escolhe uma ação aleatória
        act = np.random.randint(q_matrix.shape[1])
    else:
        # Exploit: escolhe a ação com maior valor Q
        act = np.argmax(q_matrix[state])
    return act


def epsilon_greedy_decaying(q_matrix, state, min_epsilon=0, max_epsilon=1, mode='linear', time=1, max_time=100, k=0.1):
    """
    Função que implementa o epsilon decaying para usar em epsilon_greedy
    """
    if mode == 'linear':
        epsilon = max_epsilon - (max_epsilon - min_epsilon) * (time / max_time)
        epsilon = max(min_epsilon, epsilon)
    elif mode == 'exponential':
        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-k*time)
    else:
        epsilon = max_epsilon

    # logging.info(f"epsilon: {epsilon}")

    action = epsilon_greedy(q_matrix, state, epsilon)

    return action

def softmax(q_matrix, state, temperature):
    """
    Função que implementa a estratégia Softmax
    """
    
    p_act = np.exp(q_matrix[state] / temperature) / np.sum(np.exp(q_matrix[state] / temperature))
    act = np.random.choice(range(q_matrix.shape[1]), p=p_act)
    
    return act

def upper_confidence_bound(q_matrix, state, time, N_matrix, c):
    """
    Função que implementa a estratégia Upper Confidence Bound
    """
    
    act = np.argmax(q_matrix[state] + c * np.sqrt(np.log(time) / N_matrix[state]))
    
    return act

    
def choose_best_action(q_matrix, state):
    """
    Função que retorna o índice da melhor ação (com maior valor Q)
    para o estado atual.
    """
    act = np.argmax(q_matrix[state])  # retorna índice (0-based)
    return act

"""Regra de Aprendizagem"""

def q_update(state, action, next_state, rw, q_matrix, alpha, gamma):
    """
    Atualiza a estimativa Q para o par (estado, ação) com base na recompensa observada
    e na estimativa de valor futuro.

    Parâmetros:
        state: estado atual (índice inteiro)
        action: ação tomada (índice inteiro)
        next_state: próximo estado (índice inteiro)
        rw: vetor de recompensas por estado
        q_matrix: matriz Q (numpy array 2D)
        alpha: taxa de aprendizado
        gamma: fator de desconto

    Retorna:
        Novo valor Q para o par (state, action)
    """
    estimate_q = rw[state] + gamma * np.max(q_matrix[next_state, :])
    q_value = q_matrix[state, action] + alpha * (estimate_q - q_matrix[state, action])
    return q_value

# Função para executar a política aprendida e registrar a recompensa total acumulada

def simulate_policy(q_matrix, rw):

    r_total = 0

    state = 0  # em Python usamos índice 0, então 1 em R vira 0 aqui

    terminal = True

    counter = 0
    while terminal:

        # Escolher ação com base na política aprendida
        action_trial = choose_best_action(q_matrix, state)

        # Selecionar a matriz de transição correspondente à ação
        if action_trial == 0:
            transition_state = T_up[state]
        elif action_trial == 1:
            transition_state = T_down[state]
        elif action_trial == 2:
            transition_state = T_left[state]
        elif action_trial == 3:
            transition_state = T_right[state]

        # Aplicar a ação e observar o próximo estado
        next_state = calc_action_result(state, transition_state)

        logging.info(f"{state} {actions_names[action_trial]} {next_state}")

        # Acumular recompensa
        r_total += rw[next_state]
        counter += 1

        # Atualizar estado
        state = next_state

        # Verificar se é estado terminal
        if state == 9 or state == 10:  # 10 e 11 em R = 9 e 10 em Python
            terminal = False

        if counter > 100:
            break   
    # Resultado total acumulado
    return r_total

# Função auxiliar para imprimir a política gerada a partir da matrix q

import numpy as np

def print_policy(q_matrix, actions):

    # policy: índice da ação com maior valor Q em cada estado (max.col equivalente)
    policy = np.argmax(q_matrix, axis=1)  # retorna array com índice da melhor ação por estado

    # Em Python, índice começa em 0, então ajustamos os índices usados para extrair elementos do policy:
    s1 = " ".join([actions[policy[2]], actions[policy[4]], actions[policy[7]], "+1"])
    s2 = " ".join([actions[policy[1]], "*", actions[policy[6]], "-1"])
    s3 = " ".join([actions[policy[0]], actions[policy[3]], actions[policy[5]], actions[policy[8]]])

    logging.info(f"\n{s1}\n{s2}\n{s3}")

alphas = [0.1]
gammas = [1.0]

# alphas = [0.05, 0.1, 0.15, 0.2, 0.25]
# gammas = [0.8, 0.85, 0.9, 0.95, 1.0]

exploration_strategy = "epsilon_greedy"

c = 0.5

temperature = 0.2

max_epsilon = 0.9
min_epsilon = 0.1
k = 0.05
mode = "exponential"

results = []

for n in range(1,11):
    logging.info("Iniciando iteração: ", n)

    for alpha in alphas:
        for gamma in gammas:

            logging.info("Iniciando treinamento")
            logging.info("alpha: ", alpha)
            logging.info("gamma: ", gamma)

            """**INICIALIZAÇÃO**"""

            import numpy as np

            # Nomes das ações (para referência, se quiser usar como rótulos)
            actions_names = ["UP", "DW", "LF", "RG"]

            # Inicialização da matriz Q (11 estados x 4 ações)
            q_matrix = np.zeros((11, 4))

            # Estados terminais (indexando a partir de 0 → estado 10 e 11 em R são 9 e 10 em Python)
            q_matrix[9, :] = -1    # Estado 10 (índice 9)
            q_matrix[10, :] = 1    # Estado 11 (índice 10)

            # Matriz de contagem de visitas para cada par (estado, ação) – opcional para controle
            N_matrix = np.zeros((11, 4))
            
            # Vetor de recompensas por estado
            rw = np.full(11, -0.04)
            rw[9] = -1     # Estado 10
            rw[10] = 1     # Estado 11

            """**APRENDIZADO**"""

            # Cada execução do loop representa uma exploração do ambiente
            # a partir do estado inicial, escolhendo ações aleatórias
            # até alcançar um estado terminal.
            import pandas as pd

            # número de trajetórias

            T = 100

            logging.info(f"Número de trajetórias: {T}")

            for i in range(T):

                logging.info(f"Iniciando trajetória: {i}")

                state = 0  # estado 1 em R → índice 0 em Python
                terminal = True
                counter = 0

                while terminal:
                    # Escolher uma ação aleatória
                    if exploration_strategy == "epsilon_greedy":
                        action_trial = epsilon_greedy_decaying(q_matrix, state, min_epsilon, max_epsilon, mode='linear', time=i, max_time=T, k=0.1)
                    elif exploration_strategy == "softmax":
                        action_trial = softmax(q_matrix, state, temperature)
                    elif exploration_strategy == "upper_confidence_bound":
                        action_trial = upper_confidence_bound(q_matrix, state, i, N_matrix, c)
                    else:
                        action_trial = np.random.choice([0, 1, 2, 3])  # 0: UP, 1: DW, 2: LF, 3: RG

                    # Incrementar a contagem de visitas
                    N_matrix[state, action_trial] += 1

                    # Selecionar a matriz de transição correspondente à ação escolhida
                    if action_trial == 0:
                        transition_state = T_up[state, :]
                    elif action_trial == 1:
                        transition_state = T_down[state, :]
                    elif action_trial == 2:
                        transition_state = T_left[state, :]
                    elif action_trial == 3:
                        transition_state = T_right[state, :]

                    # Simular a ação e obter o próximo estado
                    next_state = calc_action_result(state, transition_state)

                    # Mostrar a transição
                    # logging.info(f"{state} {actions_names[action_trial]} {next_state}")

                    # Atualizar o valor Q
                    q_matrix[state, action_trial] = q_update(
                        state, action_trial, next_state, rw, q_matrix, alpha, gamma
                    )

                    # Atualizar o estado atual
                    state = next_state

                    counter += 1

                    # Verifica se é estado terminal (estado 10 ou 11 → índices 9 ou 10)
                    if state == 9 or state == 10:
                        terminal = False

                    if counter > 100:
                        break
                    
            logging.info("\n")  # linha em branco entre episódios
            # sleep(1)

            q_matrix_df = pd.DataFrame(q_matrix, columns=actions_names)

            logging.info(f"{q_matrix_df}")

        
            print_policy(q_matrix, actions_names)

            """**SIMULANDO A POLÍTICA APRENDIDA**"""

            # Simulando a execução da política a partir do estado inicial ate o estado final alcançado e registrado a recompensa total obtiga pelo agente

            logging.info(f"Avaliação ")
            r_total = simulate_policy(q_matrix, rw)

            logging.info(f"Recompensa total: {r_total}")

            # results[(i, alpha, gamma)] = r_total
            results.append({
                "iteração": n,
                "alpha": alpha,
                "gamma": gamma,
                "recompensa": r_total
            })
    
results_df = pd.DataFrame(results, columns=["iteração", "alpha", "gamma", "recompensa"])
results_df.to_csv(f'rl/results_{exploration_strategy}_{mode}_30_{k}.csv', mode='w')

evaluation = results_df.pivot_table(index = ["alpha", "gamma"], values="recompensa", aggfunc = ['mean','std']).reset_index()
evaluation.columns = evaluation.columns.droplevel(1)
evaluation.to_csv(f'rl/evaluation_{exploration_strategy}_{mode}_30_{k}.csv', mode='w')
            


"""# **ITEM 1:**

* **Treinamento:** Execute o Q Learning variando os parâmetros de alpha e gamma, com cinco opções de valores para cada parâmetro. Obs.: vc pode usar como critério de parada o número de 30 trajetórias ou algum critério de convergência (sobre a matrix Q). Execute o Q Learning 10 vezes para cada combinação de alpha e gamma.

*  **Avaliação:** Para avaliar cada política aprendida, simule a execução da política por várias vezes (por exemplo, 10 vezes) e registre a média da recompensa total recebida.

* Defina assim a melhor configuração de alpha e gamma avaliada.

Tabela de resultados (exemplo):

Alpha | Gamma | Recompensa Total (Media das 100 execuções) | Desvio

0.2   | 0.1   |       xxx                                   |  yyy

0.4   | 0.3   |       xxx                                   |  yyy

0.6   | 0.5   |       xxx                                   |  yyy
etc...
"""



"""# **ITEM 2:**

* Implemente duas estratégias de exploração de estados, como eps-greedy, Boltzman ou UCB. Execute e avalie o Q Learning com cada uma das estratégias.

* Obs.: para simplificar fixe os valores de alpha e gamma obtidos no item anterior, mas se tiver tempo pode realizar experimentos adicionais. Novamente vc pode usar o número de 30 trajetórias como critério de parada.

* Obs.: A qualidade da estratégia de exploração depende de parâmetros. Avalie opções diferentes de valores.
"""