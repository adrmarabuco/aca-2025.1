# -*- coding: utf-8 -*-
"""Q Learning - Execução.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127w6sd6piMUaT1P3w9yGNWRpJmlYjCac

**MATRIZ DE TRANSIÇÃO DE ESTADOS (PARA SIMULAÇÃO DO AGENTE)**
"""

import numpy as np
import pandas as pd
import logging

# Cria uma matriz 11x11 preenchida com zeros
T_up = np.zeros((11, 11))

# Preenche a matriz com as probabilidades de transição
T_up[0, 3] = 0.1
T_up[0, 0] = 0.1
T_up[0, 1] = 0.8
T_up[1, 1] = 0.2
T_up[1, 2] = 0.8
T_up[2, 4] = 0.1
T_up[2, 2] = 0.9
T_up[3, 5] = 0.1
T_up[3, 3] = 0.8
T_up[3, 0] = 0.1
T_up[4, 7] = 0.1
T_up[4, 2] = 0.1
T_up[4, 4] = 0.8
T_up[5, 8] = 0.1
T_up[5, 3] = 0.1
T_up[5, 6] = 0.8
T_up[6, 9] = 0.1
T_up[6, 6] = 0.1
T_up[6, 7] = 0.8
T_up[7, 10] = 0.1
T_up[7, 4] = 0.1
T_up[7, 7] = 0.8
T_up[8, 8] = 0.1
T_up[8, 5] = 0.1
T_up[8, 9] = 0.8


# MATRIZ DE TRANSIÇÃO PARA A AÇÃO DOWN
T_down = np.zeros((11, 11))
T_down[0, 0] = 0.9
T_down[0, 3] = 0.1
T_down[1, 0] = 0.8
T_down[1, 1] = 0.2
T_down[2, 1] = 0.8
T_down[2, 2] = 0.1
T_down[2, 4] = 0.1
T_down[3, 0] = 0.1
T_down[3, 5] = 0.1
T_down[3, 3] = 0.8
T_down[4, 4] = 0.8
T_down[4, 2] = 0.1
T_down[4, 7] = 0.1
T_down[5, 5] = 0.8
T_down[5, 3] = 0.1
T_down[5, 8] = 0.1
T_down[6, 5] = 0.8
T_down[6, 6] = 0.1
T_down[6, 9] = 0.1
T_down[7, 6] = 0.8
T_down[7, 4] = 0.1
T_down[7, 10] = 0.1
T_down[8, 8] = 0.9
T_down[8, 5] = 0.1

# MATRIZ DE TRANSIÇÃO PARA A AÇÃO LEFT
T_left = np.zeros((11, 11))
T_left[0, 0] = 0.9
T_left[0, 1] = 0.1
T_left[1, 0] = 0.1
T_left[1, 2] = 0.1
T_left[1, 1] = 0.8
T_left[2, 2] = 0.9
T_left[2, 1] = 0.1
T_left[3, 0] = 0.8
T_left[3, 3] = 0.2
T_left[4, 2] = 0.8
T_left[4, 4] = 0.2
T_left[5, 3] = 0.8
T_left[5, 5] = 0.1
T_left[5, 6] = 0.1
T_left[6, 6] = 0.8
T_left[6, 5] = 0.1
T_left[6, 7] = 0.1
T_left[7, 4] = 0.8
T_left[7, 7] = 0.1
T_left[7, 6] = 0.1
T_left[8, 5] = 0.8
T_left[8, 8] = 0.1
T_left[8, 9] = 0.1

# MATRIZ DE TRANSIÇÃO PARA A AÇÃO RIGHT
T_right = np.zeros((11, 11))
T_right[0, 3] = 0.8
T_right[0, 0] = 0.1
T_right[0, 1] = 0.1
T_right[1, 1] = 0.8
T_right[1, 0] = 0.1
T_right[1, 2] = 0.1
T_right[2, 4] = 0.8
T_right[2, 2] = 0.1
T_right[2, 1] = 0.1
T_right[3, 5] = 0.8
T_right[3, 3] = 0.2
T_right[4, 7] = 0.8
T_right[4, 4] = 0.2
T_right[5, 8] = 0.9
T_right[5, 5] = 0.1
T_right[5, 6] = 0.1
T_right[6, 9] = 0.8
T_right[6, 5] = 0.1
T_right[6, 7] = 0.1
T_right[7, 10] = 0.8
T_right[7, 6] = 0.1
T_right[7, 7] = 0.1
T_right[8, 9] = 0.1
T_right[8, 8] = 0.9

"""**FUNÇÕES AUXILIARES**"""

# Função para simular o resultado estocástico de aplicar uma ação ao estado atual

def calc_action_result(state, transition_state):

    # Obtém os índices dos estados candidatos (com probabilidade > 0)
    cand_states = np.where(transition_state != 0)[0]
    prod_cand_states = transition_state[cand_states]

    # Ordena as probabilidades e obtém os índices ordenados
    sorted_indices = np.argsort(prod_cand_states)
    cand_states_sort = cand_states[sorted_indices]
    prod_cand_states_sort = prod_cand_states[sorted_indices]

    # Constrói a roleta acumulada
    roleta = np.cumsum(prod_cand_states_sort)

    # Gera número aleatório uniforme entre 0 e 1
    r = np.random.uniform()

    # Encontra o primeiro índice da roleta onde a probabilidade acumulada excede r
    ind = np.where(roleta > r)[0]

    return cand_states_sort[ind[0]]

def choose_best_action(q_matrix, state):
    """
    Função que retorna o índice da melhor ação (com maior valor Q)
    para o estado atual.
    """
    act = np.argmax(q_matrix[state])  # retorna índice (0-based)
    return act

"""Regra de Aprendizagem"""

def q_update(state, action, next_state, rw, q_matrix, alpha, gamma):
    """
    Atualiza a estimativa Q para o par (estado, ação) com base na recompensa observada
    e na estimativa de valor futuro.

    Parâmetros:
        state: estado atual (índice inteiro)
        action: ação tomada (índice inteiro)
        next_state: próximo estado (índice inteiro)
        rw: vetor de recompensas por estado
        q_matrix: matriz Q (numpy array 2D)
        alpha: taxa de aprendizado
        gamma: fator de desconto

    Retorna:
        Novo valor Q para o par (state, action)
    """
    estimate_q = rw[state] + gamma * np.max(q_matrix[next_state, :])
    q_value = q_matrix[state, action] + alpha * (estimate_q - q_matrix[state, action])
    return q_value

# Função para executar a política aprendida e registrar a recompensa total acumulada

def simulate_policy(q_matrix, rw):

  r_total = 0

  state = 0  # em Python usamos índice 0, então 1 em R vira 0 aqui

  terminal = True

  while terminal:

      # Escolher ação com base na política aprendida
      action_trial = choose_best_action(q_matrix, state)

      # Selecionar a matriz de transição correspondente à ação
      if action_trial == 0:
          transition_state = T_up[state]
      elif action_trial == 1:
          transition_state = T_down[state]
      elif action_trial == 2:
          transition_state = T_left[state]
      elif action_trial == 3:
          transition_state = T_right[state]

      # Aplicar a ação e observar o próximo estado
      next_state = calc_action_result(state, transition_state)

      print(f"{state} {actions_names[action_trial]} {next_state}")

      # Acumular recompensa
      r_total += rw[next_state]

      # Atualizar estado
      state = next_state

      # Verificar se é estado terminal
      if state == 9 or state == 10:  # 10 e 11 em R = 9 e 10 em Python
        terminal = False

  # Resultado total acumulado
  return r_total

# Função auxiliar para imprimir a política gerada a partir da matrix q

import numpy as np

def print_policy(q_matrix, actions):

  # policy: índice da ação com maior valor Q em cada estado (max.col equivalente)
  policy = np.argmax(q_matrix, axis=1)  # retorna array com índice da melhor ação por estado

  # Em Python, índice começa em 0, então ajustamos os índices usados para extrair elementos do policy:
  s1 = " ".join([actions[policy[2]], actions[policy[4]], actions[policy[7]], "+1"])
  s2 = " ".join([actions[policy[1]], "*", actions[policy[6]], "-1"])
  s3 = " ".join([actions[policy[0]], actions[policy[3]], actions[policy[5]], actions[policy[8]]])

  print("\n", s1, "\n", s2, "\n", s3)

"""**INICIALIZAÇÃO**"""

import numpy as np

# Nomes das ações (para referência, se quiser usar como rótulos)
actions_names = ["UP", "DW", "LF", "RG"]

# Inicialização da matriz Q (11 estados x 4 ações)
q_matrix = np.zeros((11, 4))

# Estados terminais (indexando a partir de 0 → estado 10 e 11 em R são 9 e 10 em Python)
q_matrix[9, :] = -1    # Estado 10 (índice 9)
q_matrix[10, :] = 1    # Estado 11 (índice 10)

# Matriz de contagem de visitas para cada par (estado, ação) – opcional para controle
N_matrix = np.zeros((11, 4))

# Hiperparâmetros
alpha = 0.2
gamma = 0.5

# Vetor de recompensas por estado
rw = np.full(11, -0.04)
rw[9] = -1     # Estado 10
rw[10] = 1     # Estado 11

"""**APRENDIZADO**"""

# Cada execução do loop representa uma exploração do ambiente
# a partir do estado inicial, escolhendo ações aleatórias
# até alcançar um estado terminal.
import pandas as pd

# número de trajetórias

T = 200

for i in range(T):

    state = 0  # estado 1 em R → índice 0 em Python
    terminal = True

    while terminal:
        # Escolher uma ação aleatória
        action_trial = np.random.choice([0, 1, 2, 3])  # 0: UP, 1: DW, 2: LF, 3: RG

        # Incrementar a contagem de visitas
        N_matrix[state, action_trial] += 1

        # Selecionar a matriz de transição correspondente à ação escolhida
        if action_trial == 0:
            transition_state = T_up[state, :]
        elif action_trial == 1:
            transition_state = T_down[state, :]
        elif action_trial == 2:
            transition_state = T_left[state, :]
        elif action_trial == 3:
            transition_state = T_right[state, :]

        # Simular a ação e obter o próximo estado
        next_state = calc_action_result(state, transition_state)

        # Mostrar a transição
        #### print(f"{state} {actions_names[action_trial]} {next_state}")

        # Atualizar o valor Q
        q_matrix[state, action_trial] = q_update(
            state, action_trial, next_state, rw, q_matrix, alpha, gamma
        )

        # Atualizar o estado atual
        state = next_state

        # Verifica se é estado terminal (estado 10 ou 11 → índices 9 ou 10)
        if state == 9 or state == 10:
            terminal = False

#    print("")  # linha em branco entre episódios


q_matrix_df = pd.DataFrame(q_matrix, columns=actions_names)

print(q_matrix_df)

print_policy(q_matrix, actions_names)

"""**SIMULANDO A POLÍTICA APRENDIDA**"""

# Simulando a execução da política a partir do estado inicial ate o estado final alcançado e registrado a recompensa total obtiga pelo agente

r_total = simulate_policy(q_matrix, rw)

print(r_total)

"""# **ITEM 1:**

* **Treinamento:** Execute o Q Learning variando os parâmetros de alpha e gamma, com cinco opções de valores para cada parâmetro. Obs.: vc pode usar como critério de parada o número de 30 trajetórias ou algum critério de convergência (sobre a matrix Q). Execute o Q Learning 10 vezes para cada combinação de alpha e gamma.

*  **Avaliação:** Para avaliar cada política aprendida, simule a execução da política por várias vezes (por exemplo, 10 vezes) e registre a média da recompensa total recebida.

* Defina assim a melhor configuração de alpha e gamma avaliada.

Tabela de resultados (exemplo):

Alpha | Gamma | Recompensa Total (Media das 100 execuções) | Desvio

0.2   | 0.1   |       xxx                                   |  yyy

0.4   | 0.3   |       xxx                                   |  yyy

0.6   | 0.5   |       xxx                                   |  yyy
etc...
"""



"""# **ITEM 2:**

* Implemente duas estratégias de exploração de estados, como eps-greedy, Boltzman ou UCB. Execute e avalie o Q Learning com cada uma das estratégias.

* Obs.: para simplificar fixe os valores de alpha e gamma obtidos no item anterior, mas se tiver tempo pode realizar experimentos adicionais. Novamente vc pode usar o número de 30 trajetórias como critério de parada.

* Obs.: A qualidade da estratégia de exploração depende de parâmetros. Avalie opções diferentes de valores.
"""

class Q: 
    def __init__(self, alpha, gamma):
        self.alpha = alpha
        self.gamma = gamma
        self.q_matrix = np.zeros((11, 4))
        self.N_matrix = np.zeros((11, 4))
        self.rw = np.full(11, -0.04)
        self.rw[9] = -1     # Estado 10
        self.rw[10] = 1     # Estado 11

        logging.info(f"Alpha: {self.alpha}")
        logging.info(f"Gamma: {self.gamma}")
        logging.info("" * 50)
        
    def train(self, T):
        for i in range(T):
            logging.info(f"Episódio: {i}")
            self.state = 0  # estado 1 em R → índice 0 em Python
            self.terminal = True

            while self.terminal:
                # Escolher uma ação aleatória
                self.action_trial = np.random.choice([0, 1, 2, 3])  # 0: UP, 1: DW, 2: LF, 3: RG

                # Incrementar a contagem de visitas
                self.N_matrix[self.state, self.action_trial] += 1

                # Selecionar a matriz de transição correspondente à ação escolhida
                if self.action_trial == 0:
                    transition_state = T_up[self.state, :]
                elif self.action_trial == 1:
                    transition_state = T_down[self.state, :]
                elif self.action_trial == 2:
                    transition_state = T_left[self.state, :]
                elif self.action_trial == 3:
                    transition_state = T_right[self.state, :]

                # Simular a ação e obter o próximo estado
                self.next_state = calc_action_result(self.state, transition_state)

                # Mostrar a transição
                logging.info(f"{self.state} {actions_names[self.action_trial]} {self.next_state}")

                # Atualizar o valor Q
                self.q_matrix[self.state, self.action_trial] = q_update(
                    self.state, self.action_trial, self.next_state, self.rw, self.q_matrix, self.alpha, self.gamma
                )

                # Atualizar o estado atual
                self.state = self.next_state

                # Verifica se é estado terminal (estado 10 ou 11 → índices 9 ou 10)
                if self.state == 9 or self.state == 10:
                    self.terminal = False
        
        logging.info("")  # linha em branco entre episódios

        self.q_matrix_df = pd.DataFrame(self.q_matrix, columns=actions_names)
        self.q_matrix_df.to_csv(f"q_matrix_{self.alpha}_{self.gamma}.csv")

    def evaluate(self):
        r_total = simulate_policy(self.q_matrix, self.rw)
        return r_total

if __name__ == "__main__":
    alfas = range(0.05, 0.5, 0.1)
    gammas = range(1, 0.8, 0.05)
    
    results = {}
    for i in range(1,10): 
        for alpha in alfas:
            for gamma in gammas:
                q = Q(alpha, gamma)
                q.train(30)
                r_total = q.evaluate()
                results[(i, alpha, gamma)] = r_total
    results_df = pd.DataFrame(results.items(), columns=["iteração", "alpha", "gamma", "recompensa"])
    results_df.to_csv('results.csv', if_exists='append', index=False)

    evaluation = results_df.pivot_table(index = ["alpha", "gamma"], values="recompensa", aggfunc = ['mean','std'])
    evaluation.to_csv('evaluation.csv', if_exists='replace', index=False)
            